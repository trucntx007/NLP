{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StockPrediction_SentimentAnalysis.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2roBDvi2jNu",
        "outputId": "068fde74-3c78-47f9-b287-801eca5689b2"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "from nltk.corpus import subjectivity\n",
        "from nltk.sentiment import SentimentAnalyzer\n",
        "from nltk.sentiment.util import *\n",
        "import matplotlib.pyplot as mlpt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUj2RmPD2vBs"
      },
      "source": [
        "import tweepy\n",
        "import csv\n",
        "import pandas as pd\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "up_k4lj5Jfms"
      },
      "source": [
        "# News Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtBxkD_s2z2s",
        "outputId": "2c2d71ba-d970-40ce-9426-2be6dac20295"
      },
      "source": [
        "# import data\n",
        "url = 'https://raw.githubusercontent.com/trucntx007/NLP/data/stockerbot-export.csv'\n",
        "df = pd.read_csv(url, error_bad_lines=False)\n",
        "\n",
        "df.info()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 28264 entries, 0 to 28263\n",
            "Data columns (total 8 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   id             28264 non-null  int64 \n",
            " 1   text           28264 non-null  object\n",
            " 2   timestamp      28264 non-null  object\n",
            " 3   source         28264 non-null  object\n",
            " 4   symbols        28264 non-null  object\n",
            " 5   company_names  28263 non-null  object\n",
            " 6   url            21895 non-null  object\n",
            " 7   verified       28264 non-null  bool  \n",
            "dtypes: bool(1), int64(1), object(6)\n",
            "memory usage: 1.5+ MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "b'Skipping line 731: expected 8 fields, saw 13\\nSkipping line 2836: expected 8 fields, saw 15\\nSkipping line 3058: expected 8 fields, saw 12\\nSkipping line 3113: expected 8 fields, saw 12\\nSkipping line 3194: expected 8 fields, saw 17\\nSkipping line 3205: expected 8 fields, saw 17\\nSkipping line 3255: expected 8 fields, saw 17\\nSkipping line 3520: expected 8 fields, saw 17\\nSkipping line 4078: expected 8 fields, saw 17\\nSkipping line 4087: expected 8 fields, saw 17\\nSkipping line 4088: expected 8 fields, saw 17\\nSkipping line 4499: expected 8 fields, saw 12\\n'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "40lZ2ZX7S2UV",
        "outputId": "bc277abe-b914-4dd4-94ed-b61edb438672"
      },
      "source": [
        "df.sample(10).head(5)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>source</th>\n",
              "      <th>symbols</th>\n",
              "      <th>company_names</th>\n",
              "      <th>url</th>\n",
              "      <th>verified</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3814</th>\n",
              "      <td>1017485603582238700</td>\n",
              "      <td>Firstenergy Corp $FE Stock Value Rose While Th...</td>\n",
              "      <td>Thu Jul 12 19:07:26 +0000 2018</td>\n",
              "      <td>reurope_stock</td>\n",
              "      <td>FE</td>\n",
              "      <td>FirstEnergy Corp.</td>\n",
              "      <td>https://reurope.com/2018/07/12/firstenergy-cor...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26595</th>\n",
              "      <td>1019696729602474000</td>\n",
              "      <td>Copy lucrative traders automatically with Bitc...</td>\n",
              "      <td>Wed Jul 18 21:33:40 +0000 2018</td>\n",
              "      <td>msarybsarstee</td>\n",
              "      <td>ETN</td>\n",
              "      <td>Eaton Corporation plc</td>\n",
              "      <td>https://1broker.com/?r=25023</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25884</th>\n",
              "      <td>1019684122099216400</td>\n",
              "      <td>https://t.co/0mvtSFNitP  #Nadella has done a g...</td>\n",
              "      <td>Wed Jul 18 20:43:34 +0000 2018</td>\n",
              "      <td>paranjpe12</td>\n",
              "      <td>MSFT</td>\n",
              "      <td>Microsoft Corporation</td>\n",
              "      <td>https://www.cnbc.com/2018/07/17/how-microsoft-...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23710</th>\n",
              "      <td>1019639090340229100</td>\n",
              "      <td>$TMK New Insider Filing On:  LUTEK BEN Transac...</td>\n",
              "      <td>Wed Jul 18 17:44:37 +0000 2018</td>\n",
              "      <td>filing_scanner</td>\n",
              "      <td>TMK</td>\n",
              "      <td>Torchmark Corporation</td>\n",
              "      <td>http://www.filingscanner.com/Alerts/TMK.php</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11429</th>\n",
              "      <td>1018955475914764300</td>\n",
              "      <td>$BA $JPM $EOG $GS $ORCL $BAC $V $EL $SPB $ACN ...</td>\n",
              "      <td>Mon Jul 16 20:28:11 +0000 2018</td>\n",
              "      <td>teresaarthur66</td>\n",
              "      <td>UTX</td>\n",
              "      <td>United Technologies Corporation</td>\n",
              "      <td>https://twitter.com/i/web/status/1018955475914...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        id  ... verified\n",
              "3814   1017485603582238700  ...    False\n",
              "26595  1019696729602474000  ...    False\n",
              "25884  1019684122099216400  ...    False\n",
              "23710  1019639090340229100  ...    False\n",
              "11429  1018955475914764300  ...    False\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-anZJu86p24"
      },
      "source": [
        "# News Data Prep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PadTtbbPBwqM",
        "outputId": "625dcb1a-3706-44ac-e78c-2e4e8ad63de5"
      },
      "source": [
        "# Removing special characters\n",
        "\n",
        "import re\n",
        "import copy\n",
        "\n",
        "#df_cleaned = pd.DataFrame(columns=['timestamp','text'])\n",
        "df_cleaned = df[['timestamp','text', 'symbols']]\n",
        "\n",
        "spec_cha = \"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\"\n",
        "#spec_cha = '[^A-Za-z0-9]+'\n",
        "\n",
        "\n",
        "df_cleaned['text'] = df_cleaned['text'].replace(to_replace=spec_cha, regex=True, value=' ')\n",
        "df_cleaned['text'].reset_index(drop=True)\n",
        "#df_cleaned = [df_cleaned['text'].replace(spec_cha, ' ')]\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  del sys.path[0]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        VIDEO   I was in my office  I was minding my o...\n",
              "1        The price of lumber  LB F is down 22  since hi...\n",
              "2                   Who says the American Dream is dead   \n",
              "3        Barry Silbert is extremely optimistic on bitco...\n",
              "4        How satellites avoid attacks and space junk wh...\n",
              "                               ...                        \n",
              "28259            FB   29234a9c 7f08 4d5a 985f cb1a5554ecf9\n",
              "28260                                              BTC    \n",
              "28261    RT  invest in hd   Nuff said    TEL  telcoin  ...\n",
              "28262                                              BTC    \n",
              "28263    Stellar  XLM price   0 297852 Binance registra...\n",
              "Name: text, Length: 28264, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WsAc76k7-eL",
        "outputId": "0c487bb8-1b08-4794-8691-23d9018fb9aa"
      },
      "source": [
        "# get datetime from timestamp\n",
        "df_cleaned['datetime'] = pd.to_datetime(df['timestamp']).apply(lambda x: x.date())\n",
        "\n",
        "# sort by 'symbols', 'datetime'\n",
        "df_cleaned = df_cleaned.sort_values(['symbols', 'datetime'])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "N0o_nIR29Mo-",
        "outputId": "f090d035-d67e-412c-af92-58b80355fd95"
      },
      "source": [
        "df_cleaned.head(10)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timestamp</th>\n",
              "      <th>text</th>\n",
              "      <th>symbols</th>\n",
              "      <th>datetime</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11882</th>\n",
              "      <td>Mon Jul 16 23:54:50 +0000 2018</td>\n",
              "      <td>a pa  ion du football  tait d j  connue</td>\n",
              "      <td>A</td>\n",
              "      <td>2018-07-16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11903</th>\n",
              "      <td>Tue Jul 17 00:04:26 +0000 2018</td>\n",
              "      <td>RT  ShaneOliverAMP  EZ shares  0 2  US shares ...</td>\n",
              "      <td>A</td>\n",
              "      <td>2018-07-17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11981</th>\n",
              "      <td>Tue Jul 17 00:45:27 +0000 2018</td>\n",
              "      <td>The geometric  R  matrix for affine crystals o...</td>\n",
              "      <td>A</td>\n",
              "      <td>2018-07-17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12026</th>\n",
              "      <td>Tue Jul 17 01:14:22 +0000 2018</td>\n",
              "      <td>I  AM  ONE  EXAM  BAM   IT  CRY  OUT  HUBS  A...</td>\n",
              "      <td>A</td>\n",
              "      <td>2018-07-17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12068</th>\n",
              "      <td>Tue Jul 17 01:44:22 +0000 2018</td>\n",
              "      <td>ACE  A  FIG  CRY  TOO  BIG  I  AM  MS  PEN  T...</td>\n",
              "      <td>A</td>\n",
              "      <td>2018-07-17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12201</th>\n",
              "      <td>Tue Jul 17 02:56:35 +0000 2018</td>\n",
              "      <td>The only good moment on tonight s  raw     a ha</td>\n",
              "      <td>A</td>\n",
              "      <td>2018-07-17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12218</th>\n",
              "      <td>Tue Jul 17 03:11:54 +0000 2018</td>\n",
              "      <td>nicoleyyy17 m4R M1nG    4m   p     A    0r</td>\n",
              "      <td>A</td>\n",
              "      <td>2018-07-17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12224</th>\n",
              "      <td>Tue Jul 17 03:14:38 +0000 2018</td>\n",
              "      <td>MY  INN  ATE  A  RACE  AT  A  KING  STAY  TEN...</td>\n",
              "      <td>A</td>\n",
              "      <td>2018-07-17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12259</th>\n",
              "      <td>Tue Jul 17 03:41:26 +0000 2018</td>\n",
              "      <td>RT  freshagenda2013  OK so maybe you re not as...</td>\n",
              "      <td>A</td>\n",
              "      <td>2018-07-17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12309</th>\n",
              "      <td>Tue Jul 17 04:14:20 +0000 2018</td>\n",
              "      <td>IM  OUT  A  GAS  I  ACE  ALL  KEYS  AGO  DATA...</td>\n",
              "      <td>A</td>\n",
              "      <td>2018-07-17</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                            timestamp  ...    datetime\n",
              "11882  Mon Jul 16 23:54:50 +0000 2018  ...  2018-07-16\n",
              "11903  Tue Jul 17 00:04:26 +0000 2018  ...  2018-07-17\n",
              "11981  Tue Jul 17 00:45:27 +0000 2018  ...  2018-07-17\n",
              "12026  Tue Jul 17 01:14:22 +0000 2018  ...  2018-07-17\n",
              "12068  Tue Jul 17 01:44:22 +0000 2018  ...  2018-07-17\n",
              "12201  Tue Jul 17 02:56:35 +0000 2018  ...  2018-07-17\n",
              "12218  Tue Jul 17 03:11:54 +0000 2018  ...  2018-07-17\n",
              "12224  Tue Jul 17 03:14:38 +0000 2018  ...  2018-07-17\n",
              "12259  Tue Jul 17 03:41:26 +0000 2018  ...  2018-07-17\n",
              "12309  Tue Jul 17 04:14:20 +0000 2018  ...  2018-07-17\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVky6sW3TaEq"
      },
      "source": [
        "# new dataframe to house combined text\n",
        "# text of the same company (same symbol) on a same date will be groupped together\n",
        "\n",
        "df_cleaned1 = pd.DataFrame(columns=['datetime','text','symbols'])\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5xNokw9Tv3R"
      },
      "source": [
        "# grouping text that have the same symbol to one row\n",
        "# then group by date\n",
        "indx=0\n",
        "get_tweet=\"\"\n",
        "for i in range(0,len(df_cleaned)-1):\n",
        "    get_date = df_cleaned['datetime'].iloc[i]\n",
        "    next_date = df_cleaned['datetime'].iloc[i+1]\n",
        "\n",
        "    get_symbols = df_cleaned['symbols'].iloc[i]\n",
        "    next_symbols = df_cleaned['symbols'].iloc[i+1]\n",
        "\n",
        "    if(str(get_symbols) == str(next_symbols)): \n",
        "      if(str(get_date) != str(next_date)):\n",
        "        get_tweet = df_cleaned['text'].iloc[i]\n",
        "\n",
        "        temp_df = pd.DataFrame([[get_date, get_tweet, get_symbols]]\n",
        "                               , columns = ['Date','text','symbols'])  \n",
        "        df_cleaned1 = pd.concat([df_cleaned1, temp_df], axis = 0).reset_index(drop = True)\n",
        "        \n",
        "        get_tweet=\" \"\n",
        "      else:\n",
        "        get_tweet = get_tweet + df_cleaned['text'].iloc[i]+\" \"\n",
        "    else: \n",
        "      #if (str(get_date) != str(next_date)):\n",
        "      temp_df = pd.DataFrame([[get_date, get_tweet, get_symbols]]\n",
        "                               , columns = ['Date','text','symbols'])\n",
        "      df_cleaned1 = pd.concat([df_cleaned1, temp_df], axis = 0).reset_index(drop = True)\n",
        "      get_tweet=\" \""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "YhW2vtgwlvJD",
        "outputId": "06018122-1a52-4806-9b82-86ac9dd6bf4f"
      },
      "source": [
        "df_cleaned1"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>datetime</th>\n",
              "      <th>text</th>\n",
              "      <th>symbols</th>\n",
              "      <th>Date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>a pa  ion du football  tait d j  connue</td>\n",
              "      <td>A</td>\n",
              "      <td>2018-07-16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>A repeat of 2002  Walmart may be looking to...</td>\n",
              "      <td>A</td>\n",
              "      <td>2018-07-17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>ACE  OUT  A  CGI  HE  PAY  ALL  DATA  YELP  ...</td>\n",
              "      <td>A</td>\n",
              "      <td>2018-07-18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>myhedghog Yeah I saw  AABA was selling roughl...</td>\n",
              "      <td>AABA</td>\n",
              "      <td>2018-07-11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>BABA  YAHOY  AABA</td>\n",
              "      <td>AABA</td>\n",
              "      <td>2018-07-12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2303</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Zoetis Inc  ZTS Given Average Rating of  Buy  ...</td>\n",
              "      <td>ZTS</td>\n",
              "      <td>2018-07-15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2304</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Scan results   MACD Bearish Centerline Cross t...</td>\n",
              "      <td>ZTS</td>\n",
              "      <td>2018-07-16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2305</th>\n",
              "      <td>NaN</td>\n",
              "      <td>ZTS Zoetis Inc  SEC Filing  Form 4</td>\n",
              "      <td>ZTS</td>\n",
              "      <td>2018-07-17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2306</th>\n",
              "      <td>NaN</td>\n",
              "      <td>ZTS New Insider Filing On   Fenton Andrew Tr...</td>\n",
              "      <td>ZTS</td>\n",
              "      <td>2018-07-18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2307</th>\n",
              "      <td>NaN</td>\n",
              "      <td>When you try to gauge sentiment on a  ticker b...</td>\n",
              "      <td>ticker</td>\n",
              "      <td>2018-07-12</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2308 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     datetime  ...        Date\n",
              "0         NaN  ...  2018-07-16\n",
              "1         NaN  ...  2018-07-17\n",
              "2         NaN  ...  2018-07-18\n",
              "3         NaN  ...  2018-07-11\n",
              "4         NaN  ...  2018-07-12\n",
              "...       ...  ...         ...\n",
              "2303      NaN  ...  2018-07-15\n",
              "2304      NaN  ...  2018-07-16\n",
              "2305      NaN  ...  2018-07-17\n",
              "2306      NaN  ...  2018-07-18\n",
              "2307      NaN  ...  2018-07-12\n",
              "\n",
              "[2308 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFvupwSSVPI6"
      },
      "source": [
        "# drop 'datetime' column since already created 'Date'\n",
        "df_cleaned1 = df_cleaned1.dropna(axis=1)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "lxGkLPvxrHNw",
        "outputId": "779fa033-c3b8-4026-f8b3-720b9ed2a3a9"
      },
      "source": [
        "# df_cleaned1 has tweets groupby symbols and Date\n",
        "df_cleaned1.sort_values('symbols')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>symbols</th>\n",
              "      <th>Date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>a pa  ion du football  tait d j  connue</td>\n",
              "      <td>A</td>\n",
              "      <td>2018-07-16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A repeat of 2002  Walmart may be looking to...</td>\n",
              "      <td>A</td>\n",
              "      <td>2018-07-17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ACE  OUT  A  CGI  HE  PAY  ALL  DATA  YELP  ...</td>\n",
              "      <td>A</td>\n",
              "      <td>2018-07-18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Head To Head Analysis  Unisys  UIS and Altaba...</td>\n",
              "      <td>AABA</td>\n",
              "      <td>2018-07-18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Today s million dollar PUT  options trade   AM...</td>\n",
              "      <td>AABA</td>\n",
              "      <td>2018-07-16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2299</th>\n",
              "      <td>Analyst   portfolio manager hunting ideas  Her...</td>\n",
              "      <td>ZTS</td>\n",
              "      <td>2018-07-11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2298</th>\n",
              "      <td>Abaxis  ABAX  and Zoetis Merger Deal Crosses H...</td>\n",
              "      <td>ZTS</td>\n",
              "      <td>2018-07-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2306</th>\n",
              "      <td>ZTS New Insider Filing On   Fenton Andrew Tr...</td>\n",
              "      <td>ZTS</td>\n",
              "      <td>2018-07-18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2301</th>\n",
              "      <td>ZTS the bull pattern is confirmed  amp  a BUY...</td>\n",
              "      <td>ZTS</td>\n",
              "      <td>2018-07-13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2307</th>\n",
              "      <td>When you try to gauge sentiment on a  ticker b...</td>\n",
              "      <td>ticker</td>\n",
              "      <td>2018-07-12</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2308 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   text symbols        Date\n",
              "0           a pa  ion du football  tait d j  connue           A  2018-07-16\n",
              "1        A repeat of 2002  Walmart may be looking to...       A  2018-07-17\n",
              "2       ACE  OUT  A  CGI  HE  PAY  ALL  DATA  YELP  ...       A  2018-07-18\n",
              "10     Head To Head Analysis  Unisys  UIS and Altaba...    AABA  2018-07-18\n",
              "8     Today s million dollar PUT  options trade   AM...    AABA  2018-07-16\n",
              "...                                                 ...     ...         ...\n",
              "2299  Analyst   portfolio manager hunting ideas  Her...     ZTS  2018-07-11\n",
              "2298  Abaxis  ABAX  and Zoetis Merger Deal Crosses H...     ZTS  2018-07-10\n",
              "2306    ZTS New Insider Filing On   Fenton Andrew Tr...     ZTS  2018-07-18\n",
              "2301   ZTS the bull pattern is confirmed  amp  a BUY...     ZTS  2018-07-13\n",
              "2307  When you try to gauge sentiment on a  ticker b...  ticker  2018-07-12\n",
              "\n",
              "[2308 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MG9Root-CX0I"
      },
      "source": [
        "# Split timestamp\n",
        "# df_cleaned['timestamp'] = pd.to_datetime(df_cleaned['timestamp'],format='%Y-%m-%d')\n",
        "\n",
        "# df_cleaned[['dayofweek','month','day','time','timezone', 'year']] = df_cleaned.timestamp.str.split(expand=True)\n",
        "# df_cleaned[['hour','minute','second']] = df_cleaned.time.str.split(':',expand=True)\n",
        "\n",
        "# df_cleaned['datetime'] = df_cleaned[['year', 'month', 'day']].agg('-'.join, axis=1)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHgYjqnKI_k2"
      },
      "source": [
        "# Stock Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8rhDuEHI-F7"
      },
      "source": [
        "stocks = pd.read_csv('https://raw.githubusercontent.com/trucntx007/NLP/main/stocks_cleaned.csv')\n",
        "# Define the ticker list\n",
        "\n",
        "tickers_list = []\n",
        "\n",
        "for i in range(len(stocks)):\n",
        "  tickers = stocks['ticker'][i]\n",
        "  tickers_list.append(tickers)\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QByk1S29rUho"
      },
      "source": [
        "tickers_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwAzQGGctxOC",
        "outputId": "57a12253-0625-4883-a639-d3fe31cd76c1"
      },
      "source": [
        "!pip install yfinance"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yfinance\n",
            "  Downloading yfinance-0.1.67-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from yfinance) (0.0.10)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.19.5)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.7/dist-packages (from yfinance) (2.23.0)\n",
            "Collecting lxml>=4.5.1\n",
            "  Downloading lxml-4.6.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.3 MB 7.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24->yfinance) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24->yfinance) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24->yfinance) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (1.24.3)\n",
            "Installing collected packages: lxml, yfinance\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 4.2.6\n",
            "    Uninstalling lxml-4.2.6:\n",
            "      Successfully uninstalled lxml-4.2.6\n",
            "Successfully installed lxml-4.6.4 yfinance-0.1.67\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3ngFT6Ttdc7",
        "outputId": "7be692ac-07df-4bf6-fd5f-fead39811a2b"
      },
      "source": [
        "# Fetch data from Yahoo Finance\n",
        "import yfinance as yf\n",
        "\n",
        "data = yf.download(tickers_list, start='2018-7-9',end='2018-7-19')['Adj Close']\n",
        "\n",
        "# Drop cols with no data\n",
        "data = data.dropna(axis=1)\n",
        "\n",
        "# Print first 5 rows of the data\n",
        "print(data.head())\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*********************100%***********************]  583 of 583 completed\n",
            "\n",
            "65 Failed downloads:\n",
            "- VAR: No data found, symbol may be delisted\n",
            "- AMTD: No data found, symbol may be delisted\n",
            "- GG: No data found, symbol may be delisted\n",
            "- TMK: No data found, symbol may be delisted\n",
            "- SPN: No data found, symbol may be delisted\n",
            "- AGN: No data found, symbol may be delisted\n",
            "- WYN: No data found for this date range, symbol may be delisted\n",
            "- DPS: No data found for this date range, symbol may be delisted\n",
            "- CHK: Data doesn't exist for startDate = 1531094400, endDate = 1531958400\n",
            "- HCP: No data found for this date range, symbol may be delisted\n",
            "- RTN: No data found, symbol may be delisted\n",
            "- CTL: No data found, symbol may be delisted\n",
            "- GGP: No data found for this date range, symbol may be delisted\n",
            "- LUK: No data found for this date range, symbol may be delisted\n",
            "- S: Data doesn't exist for startDate = 1531094400, endDate = 1531958400\n",
            "- GLUU: No data found, symbol may be delisted\n",
            "- FOXA: Data doesn't exist for startDate = 1531094400, endDate = 1531958400\n",
            "- HTZ: Data doesn't exist for startDate = 1531094400, endDate = 1531958400\n",
            "- ABX: No data found for this date range, symbol may be delisted\n",
            "- LB: No data found, symbol may be delisted\n",
            "- MON: Data doesn't exist for startDate = 1531094400, endDate = 1531958400\n",
            "- JEC: No data found, symbol may be delisted\n",
            "- CY: No data found, symbol may be delisted\n",
            "- MYL: No data found, symbol may be delisted\n",
            "- DLPH: No data found, symbol may be delisted\n",
            "- AABA: No data found, symbol may be delisted\n",
            "- AKS: No data found, symbol may be delisted\n",
            "- FLIR: No data found, symbol may be delisted\n",
            "- WFT: No data found, symbol may be delisted\n",
            "- CXO: No data found, symbol may be delisted\n",
            "- FNSR: No data found, symbol may be delisted\n",
            "- APC: No data found, symbol may be delisted\n",
            "- SYMC: No data found, symbol may be delisted\n",
            "- ALXN: No data found, symbol may be delisted\n",
            "- ETFC: No data found, symbol may be delisted\n",
            "- MNK: No data found, symbol may be delisted\n",
            "- FOX: Data doesn't exist for startDate = 1531094400, endDate = 1531958400\n",
            "- TWX: No data found for this date range, symbol may be delisted\n",
            "- BBT: No data found, symbol may be delisted\n",
            "- ARRY: Data doesn't exist for startDate = 1531094400, endDate = 1531958400\n",
            "- CBS: No data found, symbol may be delisted\n",
            "- HDS: No data found, symbol may be delisted\n",
            "- ESV: No data found, symbol may be delisted\n",
            "- CRZO: No data found, symbol may be delisted\n",
            "- HRS: No data found, symbol may be delisted\n",
            "- VIAB: No data found, symbol may be delisted\n",
            "- PAH: No data found for this date range, symbol may be delisted\n",
            "- RHT: No data found, symbol may be delisted\n",
            "- FMSA: No data found for this date range, symbol may be delisted\n",
            "- DCIX: No data found, symbol may be delisted\n",
            "- XL: Data doesn't exist for startDate = 1531094400, endDate = 1531958400\n",
            "- BHGE: No data found, symbol may be delisted\n",
            "- TIF: No data found, symbol may be delisted\n",
            "- CELG: No data found, symbol may be delisted\n",
            "- ETE: Data doesn't exist for startDate = 1531094400, endDate = 1531958400\n",
            "- ARNC: Data doesn't exist for startDate = 1531094400, endDate = 1531958400\n",
            "- WPX: No data found, symbol may be delisted\n",
            "- JCP: No data found, symbol may be delisted\n",
            "- UTX: No data found, symbol may be delisted\n",
            "- NBL: No data found, symbol may be delisted\n",
            "- STI: No data found, symbol may be delisted\n",
            "- TSS: No data found, symbol may be delisted\n",
            "- PX: Data doesn't exist for startDate = 1531094400, endDate = 1531958400\n",
            "- VRX: No data found for this date range, symbol may be delisted\n",
            "- KORS: None\n",
            "                    A        AAL       AAOI  ...       ZION  ZNGA        ZTS\n",
            "Date                                         ...                            \n",
            "2018-07-09  61.546665  38.477581  45.330002  ...  48.723938  4.24  85.723831\n",
            "2018-07-10  62.082947  38.291603  45.790001  ...  48.263168  4.19  84.479462\n",
            "2018-07-11  61.293140  35.198521  45.759998  ...  48.001163  4.26  82.794174\n",
            "2018-07-12  61.868435  35.560684  48.799999  ...  47.477158  4.40  83.744606\n",
            "2018-07-13  61.800171  36.333954  47.779999  ...  46.763416  4.34  84.401085\n",
            "\n",
            "[5 rows x 518 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqlkLAv0HkBQ"
      },
      "source": [
        "# add column datetime\n",
        "data[data.index.name] = data.index"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PJMfLDCC_2a"
      },
      "source": [
        "# Transpose stock dataset\n",
        "data_T = data\n",
        "data_T = data_T.transpose()\n",
        "data_T['symbols'] = data_T.index"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "0PhbW01TDKhP",
        "outputId": "3cd23f7a-ac97-4af4-a472-07afb2662e6f"
      },
      "source": [
        "# price of a stock during the period\n",
        "data_Tr = data_T.melt(id_vars=['symbols'],var_name=\"Date\", value_name='Price')\n",
        "data_Tr.sort_values(['symbols', 'Date'])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>symbols</th>\n",
              "      <th>Date</th>\n",
              "      <th>Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A</td>\n",
              "      <td>2018-07-09</td>\n",
              "      <td>61.5467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>519</th>\n",
              "      <td>A</td>\n",
              "      <td>2018-07-10</td>\n",
              "      <td>62.0829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1038</th>\n",
              "      <td>A</td>\n",
              "      <td>2018-07-11</td>\n",
              "      <td>61.2931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1557</th>\n",
              "      <td>A</td>\n",
              "      <td>2018-07-12</td>\n",
              "      <td>61.8684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2076</th>\n",
              "      <td>A</td>\n",
              "      <td>2018-07-13</td>\n",
              "      <td>61.8002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2074</th>\n",
              "      <td>ZTS</td>\n",
              "      <td>2018-07-12</td>\n",
              "      <td>83.7446</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2593</th>\n",
              "      <td>ZTS</td>\n",
              "      <td>2018-07-13</td>\n",
              "      <td>84.4011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3112</th>\n",
              "      <td>ZTS</td>\n",
              "      <td>2018-07-16</td>\n",
              "      <td>82.8726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3631</th>\n",
              "      <td>ZTS</td>\n",
              "      <td>2018-07-17</td>\n",
              "      <td>84.0385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4150</th>\n",
              "      <td>ZTS</td>\n",
              "      <td>2018-07-18</td>\n",
              "      <td>84.2149</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4152 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     symbols       Date    Price\n",
              "0          A 2018-07-09  61.5467\n",
              "519        A 2018-07-10  62.0829\n",
              "1038       A 2018-07-11  61.2931\n",
              "1557       A 2018-07-12  61.8684\n",
              "2076       A 2018-07-13  61.8002\n",
              "...      ...        ...      ...\n",
              "2074     ZTS 2018-07-12  83.7446\n",
              "2593     ZTS 2018-07-13  84.4011\n",
              "3112     ZTS 2018-07-16  82.8726\n",
              "3631     ZTS 2018-07-17  84.0385\n",
              "4150     ZTS 2018-07-18  84.2149\n",
              "\n",
              "[4152 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnN7MDT-xW5r"
      },
      "source": [
        "# get list of symbols\n",
        "tickers_list = pd.Series(data_Tr['symbols'].unique())\n",
        "tickers_list.to_csv('tickerlist.csv')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P18kKk0WvB_a"
      },
      "source": [
        "# save stock prices to StockData.csv\n",
        "data.to_csv('StockData.csv')"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb-h_-BqAb4U"
      },
      "source": [
        "# Combine datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOY84PAoAtMR"
      },
      "source": [
        "# add column 'Price' to df_cleaned1\n",
        "df_cleaned1['Price'] = ''"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "vRkpBGt6H94C",
        "outputId": "9e9fee39-9c73-403b-8a5f-fe07c4767090"
      },
      "source": [
        "df_cleaned1.sort_values('symbols')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>symbols</th>\n",
              "      <th>Date</th>\n",
              "      <th>Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>a pa  ion du football  tait d j  connue</td>\n",
              "      <td>A</td>\n",
              "      <td>2018-07-16</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A repeat of 2002  Walmart may be looking to...</td>\n",
              "      <td>A</td>\n",
              "      <td>2018-07-17</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ACE  OUT  A  CGI  HE  PAY  ALL  DATA  YELP  ...</td>\n",
              "      <td>A</td>\n",
              "      <td>2018-07-18</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Head To Head Analysis  Unisys  UIS and Altaba...</td>\n",
              "      <td>AABA</td>\n",
              "      <td>2018-07-18</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Today s million dollar PUT  options trade   AM...</td>\n",
              "      <td>AABA</td>\n",
              "      <td>2018-07-16</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2299</th>\n",
              "      <td>Analyst   portfolio manager hunting ideas  Her...</td>\n",
              "      <td>ZTS</td>\n",
              "      <td>2018-07-11</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2298</th>\n",
              "      <td>Abaxis  ABAX  and Zoetis Merger Deal Crosses H...</td>\n",
              "      <td>ZTS</td>\n",
              "      <td>2018-07-10</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2306</th>\n",
              "      <td>ZTS New Insider Filing On   Fenton Andrew Tr...</td>\n",
              "      <td>ZTS</td>\n",
              "      <td>2018-07-18</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2301</th>\n",
              "      <td>ZTS the bull pattern is confirmed  amp  a BUY...</td>\n",
              "      <td>ZTS</td>\n",
              "      <td>2018-07-13</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2307</th>\n",
              "      <td>When you try to gauge sentiment on a  ticker b...</td>\n",
              "      <td>ticker</td>\n",
              "      <td>2018-07-12</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2308 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   text  ... Price\n",
              "0           a pa  ion du football  tait d j  connue      ...      \n",
              "1        A repeat of 2002  Walmart may be looking to...  ...      \n",
              "2       ACE  OUT  A  CGI  HE  PAY  ALL  DATA  YELP  ...  ...      \n",
              "10     Head To Head Analysis  Unisys  UIS and Altaba...  ...      \n",
              "8     Today s million dollar PUT  options trade   AM...  ...      \n",
              "...                                                 ...  ...   ...\n",
              "2299  Analyst   portfolio manager hunting ideas  Her...  ...      \n",
              "2298  Abaxis  ABAX  and Zoetis Merger Deal Crosses H...  ...      \n",
              "2306    ZTS New Insider Filing On   Fenton Andrew Tr...  ...      \n",
              "2301   ZTS the bull pattern is confirmed  amp  a BUY...  ...      \n",
              "2307  When you try to gauge sentiment on a  ticker b...  ...      \n",
              "\n",
              "[2308 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ip4pA_pKA0Yy"
      },
      "source": [
        "# append 'Price' from Stock Dataset to News Dataset\n",
        "for i in range (0,len(df_cleaned1)):\n",
        "    for j in range (0,len(data_Tr)):\n",
        "        get_tweet_date = df_cleaned1['Date'].iloc[i]\n",
        "        get_stock_date = (data_Tr['Date'].iloc[j]).date() # get rid of 00:00:00\n",
        "        \n",
        "        get_tweet_symbol = df_cleaned1['symbols'].iloc[i]\n",
        "        get_stock_symbol = data_Tr['symbols'].iloc[j]\n",
        "\n",
        "        if(str(get_tweet_symbol) == str(get_stock_symbol) and \n",
        "           (str(get_stock_date) == str(get_tweet_date))):\n",
        "            #print(get_stock_date,\" \",get_tweet_date)\n",
        "            df_cleaned1['Price'].iloc[i] = int(data_Tr['Price'][j])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2jmq4co0SrP"
      },
      "source": [
        "# fill missing 'Price' with the most recent price\n",
        "for i in range(len(df_cleaned1)):\n",
        "  if df_cleaned1['Price'].iloc[i] == '':\n",
        "    df_cleaned1['Price'].iloc[i] = df_cleaned1['Price'].iloc[i-1] "
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHP2qgbmP1hH"
      },
      "source": [
        "combined_data = df_cleaned1\n",
        "combined_data.to_csv('combined_data.csv')"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcnPXyegrzrw"
      },
      "source": [
        "# convert 'Price' to integer\n",
        "combined_data['Price'] = combined_data['Price'].apply(np.int64)\n",
        "\n",
        "# adding columns for sentiment analysis\n",
        "combined_data['Emotion'] = ''\n",
        "combined_data['Negative'] = ''\n",
        "combined_data['Neutral'] = ''\n",
        "combined_data['Positive'] = ''\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnTSJmx53GjT",
        "outputId": "1742b47d-6c1e-412d-8c45-51e5b51c5d1a"
      },
      "source": [
        "# Sentiment Analysis with vader\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlU4B2Ei3RtB",
        "outputId": "ca536514-5b6c-4617-98a2-1e2ea2945012"
      },
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import unicodedata\n",
        "sentiment_i_a = SentimentIntensityAnalyzer()\n",
        "for indexx, row in combined_data.T.iteritems():\n",
        "    try:\n",
        "        sentence_i = unicodedata.normalize('NFKD', combined_data.loc[indexx, 'text'])\n",
        "        sentence_sentiment = sentiment_i_a.polarity_scores(sentence_i)\n",
        "        combined_data['Emotion'].iloc[indexx] = sentence_sentiment['compound']\n",
        "        combined_data['Negative'].iloc[indexx] = sentence_sentiment['neg']\n",
        "        combined_data['Neutral'].iloc[indexx] = sentence_sentiment['neu']\n",
        "        combined_data['Positive'].iloc[indexx] = sentence_sentiment['compound']\n",
        "        \n",
        "    except TypeError:\n",
        "        print (stocks_dataf.loc[indexx, 'text'])\n",
        "        print (indexx)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  iloc._setitem_with_indexer(indexer, value)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        },
        "id": "Ip6GU_1T4SD-",
        "outputId": "bc017a78-e7cc-44de-a98f-e63b4360ea2e"
      },
      "source": [
        "combined_data"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>symbols</th>\n",
              "      <th>Date</th>\n",
              "      <th>Price</th>\n",
              "      <th>Emotion</th>\n",
              "      <th>Negative</th>\n",
              "      <th>Neutral</th>\n",
              "      <th>Positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>a pa  ion du football  tait d j  connue</td>\n",
              "      <td>A</td>\n",
              "      <td>2018-07-16</td>\n",
              "      <td>61</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A repeat of 2002  Walmart may be looking to...</td>\n",
              "      <td>A</td>\n",
              "      <td>2018-07-17</td>\n",
              "      <td>61</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ACE  OUT  A  CGI  HE  PAY  ALL  DATA  YELP  ...</td>\n",
              "      <td>A</td>\n",
              "      <td>2018-07-18</td>\n",
              "      <td>61</td>\n",
              "      <td>0.9657</td>\n",
              "      <td>0.033</td>\n",
              "      <td>0.887</td>\n",
              "      <td>0.9657</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>myhedghog Yeah I saw  AABA was selling roughl...</td>\n",
              "      <td>AABA</td>\n",
              "      <td>2018-07-11</td>\n",
              "      <td>61</td>\n",
              "      <td>0.296</td>\n",
              "      <td>0</td>\n",
              "      <td>0.879</td>\n",
              "      <td>0.296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>BABA  YAHOY  AABA</td>\n",
              "      <td>AABA</td>\n",
              "      <td>2018-07-12</td>\n",
              "      <td>61</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2303</th>\n",
              "      <td>Zoetis Inc  ZTS Given Average Rating of  Buy  ...</td>\n",
              "      <td>ZTS</td>\n",
              "      <td>2018-07-15</td>\n",
              "      <td>84</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2304</th>\n",
              "      <td>Scan results   MACD Bearish Centerline Cross t...</td>\n",
              "      <td>ZTS</td>\n",
              "      <td>2018-07-16</td>\n",
              "      <td>82</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2305</th>\n",
              "      <td>ZTS Zoetis Inc  SEC Filing  Form 4</td>\n",
              "      <td>ZTS</td>\n",
              "      <td>2018-07-17</td>\n",
              "      <td>84</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2306</th>\n",
              "      <td>ZTS New Insider Filing On   Fenton Andrew Tr...</td>\n",
              "      <td>ZTS</td>\n",
              "      <td>2018-07-18</td>\n",
              "      <td>84</td>\n",
              "      <td>-0.0258</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.943</td>\n",
              "      <td>-0.0258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2307</th>\n",
              "      <td>When you try to gauge sentiment on a  ticker b...</td>\n",
              "      <td>ticker</td>\n",
              "      <td>2018-07-12</td>\n",
              "      <td>84</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2308 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   text  ... Positive\n",
              "0           a pa  ion du football  tait d j  connue      ...        0\n",
              "1        A repeat of 2002  Walmart may be looking to...  ...        0\n",
              "2       ACE  OUT  A  CGI  HE  PAY  ALL  DATA  YELP  ...  ...   0.9657\n",
              "3      myhedghog Yeah I saw  AABA was selling roughl...  ...    0.296\n",
              "4                                   BABA  YAHOY  AABA    ...        0\n",
              "...                                                 ...  ...      ...\n",
              "2303  Zoetis Inc  ZTS Given Average Rating of  Buy  ...  ...        0\n",
              "2304  Scan results   MACD Bearish Centerline Cross t...  ...        0\n",
              "2305               ZTS Zoetis Inc  SEC Filing  Form 4    ...        0\n",
              "2306    ZTS New Insider Filing On   Fenton Andrew Tr...  ...  -0.0258\n",
              "2307  When you try to gauge sentiment on a  ticker b...  ...        0\n",
              "\n",
              "[2308 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYcD2EzyM-Bw"
      },
      "source": [
        "# NLP Text Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6z8dSQnNLtY",
        "outputId": "bb0b3f92-b8bf-4155-af20-c056b47c15da"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "import ssl\n",
        " \n",
        "\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "nltk.download('wordnet') \n",
        "from nltk.stem.wordnet import WordNetLemmatizer"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvTZdKv3PisD",
        "outputId": "d2653f87-70f1-4426-89af-83f386547640"
      },
      "source": [
        "!pip install langdetect"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[?25l\r\u001b[K     |▍                               | 10 kB 17.5 MB/s eta 0:00:01\r\u001b[K     |▊                               | 20 kB 23.0 MB/s eta 0:00:01\r\u001b[K     |█                               | 30 kB 18.7 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 40 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 51 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██                              | 61 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 71 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 81 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |███                             | 92 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 102 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 112 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████                            | 122 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 133 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 143 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████                           | 153 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 163 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 174 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████                          | 184 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 194 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 204 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████                         | 215 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 225 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 235 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████                        | 245 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 256 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 266 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 276 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 286 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 296 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 307 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 317 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 327 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 337 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 348 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 358 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 368 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 378 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 389 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 399 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 409 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 419 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 430 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 440 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 450 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 460 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 471 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 481 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 491 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 501 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 512 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 522 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 532 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 542 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 552 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 563 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 573 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 583 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 593 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 604 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 614 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 624 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 634 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 645 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 655 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 665 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 675 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 686 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 696 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 706 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 716 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 727 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 737 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 747 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 757 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 768 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 778 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 788 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 798 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 808 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 819 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 829 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 839 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 849 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 860 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 870 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 880 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 890 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 901 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 911 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 921 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 931 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 942 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 952 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 962 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 972 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 981 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from langdetect) (1.15.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=7630c11631bef80b079e040f229fe638502f75bd44e9d464a1cbfe8a0bba7720\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3lDr9pNNSif"
      },
      "source": [
        "# Remove Stop words\n",
        "freq = pd.Series(' '.join(df_cleaned['text']).lower().split()).value_counts()[:20]\n",
        "freq\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "stop_words = stop_words.union(freq.index.tolist())\n",
        "extra_words = ['amp', 'rt']\n",
        "stop_words = stop_words.union(extra_words)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2JEmukFPvfb"
      },
      "source": [
        "# Processing language \n",
        "\n",
        "from langdetect import detect_langs\n",
        "\n",
        "#check for valid string only to detect languages\n",
        "TextValid=[]\n",
        "\n",
        "for i in range(len(df_cleaned)):\n",
        "    TextValid.append(bool(re.match('^(?=.*[a-zA-Z])', df_cleaned.iloc[i,0])))\n",
        "    \n",
        "df_cleaned['valid'] = TextValid\n",
        "#print(len(df_cleaned[df_cleaned['valid']==False]))\n",
        "#print(len(df_cleaned[df_cleaned['valid']==True]))\n",
        "\n",
        "# Detect languages for each text\n",
        "languages = []\n",
        "\n",
        "# Loop over the sentences in the data and detect their language\n",
        "for row in range(len(df_cleaned)):\n",
        "    languages.append(detect_langs(df_cleaned.iloc[row, 0]))\n",
        "    \n",
        "languages = [str(lang).split(':')[0][1:] for lang in languages] \n",
        "\n",
        "# Assign the list to a new feature \n",
        "df_cleaned['language'] = languages\n",
        "\n",
        "# count the languages in the data\n",
        "df_cleaned['language'].value_counts()\n",
        "\n",
        "# keep EN Only\n",
        "df_cleaned = df_cleaned[df_cleaned['language']=='en']"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        },
        "id": "T_4oMpHzRo8_",
        "outputId": "bfe7d2f5-496e-4098-eae4-92e999db6a7b"
      },
      "source": [
        "df_cleaned"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timestamp</th>\n",
              "      <th>text</th>\n",
              "      <th>symbols</th>\n",
              "      <th>datetime</th>\n",
              "      <th>valid</th>\n",
              "      <th>language</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11882</th>\n",
              "      <td>Mon Jul 16 23:54:50 +0000 2018</td>\n",
              "      <td>a pa  ion du football  tait d j  connue</td>\n",
              "      <td>A</td>\n",
              "      <td>2018-07-16</td>\n",
              "      <td>True</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11903</th>\n",
              "      <td>Tue Jul 17 00:04:26 +0000 2018</td>\n",
              "      <td>RT  ShaneOliverAMP  EZ shares  0 2  US shares ...</td>\n",
              "      <td>A</td>\n",
              "      <td>2018-07-17</td>\n",
              "      <td>True</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11981</th>\n",
              "      <td>Tue Jul 17 00:45:27 +0000 2018</td>\n",
              "      <td>The geometric  R  matrix for affine crystals o...</td>\n",
              "      <td>A</td>\n",
              "      <td>2018-07-17</td>\n",
              "      <td>True</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12026</th>\n",
              "      <td>Tue Jul 17 01:14:22 +0000 2018</td>\n",
              "      <td>I  AM  ONE  EXAM  BAM   IT  CRY  OUT  HUBS  A...</td>\n",
              "      <td>A</td>\n",
              "      <td>2018-07-17</td>\n",
              "      <td>True</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12068</th>\n",
              "      <td>Tue Jul 17 01:44:22 +0000 2018</td>\n",
              "      <td>ACE  A  FIG  CRY  TOO  BIG  I  AM  MS  PEN  T...</td>\n",
              "      <td>A</td>\n",
              "      <td>2018-07-17</td>\n",
              "      <td>True</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22909</th>\n",
              "      <td>Wed Jul 18 16:33:47 +0000 2018</td>\n",
              "      <td>Analysts Set Zoetis Inc  ZTS Target Price at  ...</td>\n",
              "      <td>ZTS</td>\n",
              "      <td>2018-07-18</td>\n",
              "      <td>True</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23078</th>\n",
              "      <td>Wed Jul 18 16:49:49 +0000 2018</td>\n",
              "      <td>Zoetis Inc  ZTS to Issue Quarterly Dividend of...</td>\n",
              "      <td>ZTS</td>\n",
              "      <td>2018-07-18</td>\n",
              "      <td>True</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23097</th>\n",
              "      <td>Wed Jul 18 16:50:59 +0000 2018</td>\n",
              "      <td>Zoetis Inc  ZTS to Issue Quarterly Dividend of...</td>\n",
              "      <td>ZTS</td>\n",
              "      <td>2018-07-18</td>\n",
              "      <td>True</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23696</th>\n",
              "      <td>Wed Jul 18 17:43:22 +0000 2018</td>\n",
              "      <td>ZTS New Insider Filing On   Fenton Andrew Tra...</td>\n",
              "      <td>ZTS</td>\n",
              "      <td>2018-07-18</td>\n",
              "      <td>True</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3369</th>\n",
              "      <td>Thu Jul 12 14:28:55 +0000 2018</td>\n",
              "      <td>When you try to gauge sentiment on a  ticker b...</td>\n",
              "      <td>ticker</td>\n",
              "      <td>2018-07-12</td>\n",
              "      <td>True</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>22487 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                            timestamp  ... language\n",
              "11882  Mon Jul 16 23:54:50 +0000 2018  ...       en\n",
              "11903  Tue Jul 17 00:04:26 +0000 2018  ...       en\n",
              "11981  Tue Jul 17 00:45:27 +0000 2018  ...       en\n",
              "12026  Tue Jul 17 01:14:22 +0000 2018  ...       en\n",
              "12068  Tue Jul 17 01:44:22 +0000 2018  ...       en\n",
              "...                               ...  ...      ...\n",
              "22909  Wed Jul 18 16:33:47 +0000 2018  ...       en\n",
              "23078  Wed Jul 18 16:49:49 +0000 2018  ...       en\n",
              "23097  Wed Jul 18 16:50:59 +0000 2018  ...       en\n",
              "23696  Wed Jul 18 17:43:22 +0000 2018  ...       en\n",
              "3369   Thu Jul 12 14:28:55 +0000 2018  ...       en\n",
              "\n",
              "[22487 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwpxuXVcOCie",
        "outputId": "0748a4d5-fbd7-4415-92e5-50c9b3e6d326"
      },
      "source": [
        "corpus = []\n",
        "for i in df_cleaned.index:\n",
        "    #Remove punctuations\n",
        "    text = re.sub('[^a-zA-Z]', ' ', df_cleaned['text'][i])\n",
        "    \n",
        "    #Convert to lowercase\n",
        "    text = text.lower()\n",
        "    \n",
        "    #remove tags\n",
        "    text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
        "    \n",
        "    # remove special characters and digits\n",
        "    text = re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
        "    text = text.replace(\"\\n\",\"\")\n",
        "   \n",
        "    ##Convert to list from string\n",
        "    text = text.split()\n",
        "    \n",
        "    ##Stemming\n",
        "    ps=PorterStemmer()    #Lemmatisation\n",
        "    lem = WordNetLemmatizer()\n",
        "    text = [lem.lemmatize(word) for word in text if not word in  \n",
        "            stop_words] \n",
        "    df_cleaned['keywords'] = pd.Series(text)\n",
        "    text = \" \".join(text)\n",
        "    corpus.append(text)\n",
        "    \n",
        "pd.Series(corpus).sample(20).head(20)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19605    estate asset watch today outsized price move s...\n",
              "2402     evening tweeted mid cap check gambiste top hel...\n",
              "16908    see cdix offer msft amzn fb goog jnj jpm googo...\n",
              "17246                             g plc long term investor\n",
              "11476    keefe bruyette wood lower invesco ivz market p...\n",
              "22220        patience value name xrx make back zone beyond\n",
              "22006    xec open interest maturity high put high call ...\n",
              "20358                                txt premarket winning\n",
              "13116                                         miniapps mat\n",
              "2484               analyst see eps avalonbay community avb\n",
              "14362                        mtb new sec filing mtb form k\n",
              "10920    illumina ilmn reach new month high analyst upg...\n",
              "16382    pultegroup phm v tri pointe group tph financia...\n",
              "19715                broker issue forecast mobile u q tmus\n",
              "22486    try gauge sentiment ticker come search crypto ...\n",
              "15420    spy break keep eye amzn nflx fb aapl googl tsl...\n",
              "12194    join u signal multiple paid group one join u g...\n",
              "732      today archer daniel midland company adm report...\n",
              "11286    ajaydevfan kashur bahot kiye ham dono par saja...\n",
              "2602     yoyow currently worth register binance receive...\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZ3TTAzkJGGn"
      },
      "source": [
        "# Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gVzmqMdX1Ws",
        "outputId": "6cc0c0b1-07fb-4158-a44e-a2a7a9cf24ee"
      },
      "source": [
        "!pip install textblob"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaR6qM5gT1Jq"
      },
      "source": [
        "# Detect Emotions for each text Form TextBlob Library\n",
        "from textblob import TextBlob\n",
        "\n",
        "detectEmotion = []\n",
        "detectPolarity = []\n",
        "\n",
        "for txt in corpus:\n",
        "    analysis = TextBlob(txt)\n",
        "    Polarity = analysis.sentiment.polarity\n",
        "    \n",
        "    if Polarity < 0:\n",
        "        emotion = '2'  #Negative\n",
        "    elif Polarity > 0: \n",
        "        emotion = '1'  #Positive\n",
        "    else:\n",
        "        emotion = '0'  #Neutral\n",
        "        \n",
        "    detectEmotion.append(emotion)\n",
        "    detectPolarity.append(Polarity)\n",
        "    \n",
        "df_cleaned['Polarity'] = detectPolarity\n",
        "df_cleaned['Emotion'] = detectEmotion"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 943
        },
        "id": "NCzByHIFWa73",
        "outputId": "66c324f2-51e3-4c11-93ce-1d4442ee1282"
      },
      "source": [
        "df_cleaned.sort_values(by = ['timestamp'])\n",
        "#jul-9-2018\n",
        "#jul-18"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timestamp</th>\n",
              "      <th>text</th>\n",
              "      <th>symbols</th>\n",
              "      <th>datetime</th>\n",
              "      <th>valid</th>\n",
              "      <th>language</th>\n",
              "      <th>keywords</th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Emotion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>Mon Jul 09 00:15:27 +0000 2018</td>\n",
              "      <td>NI high OI range is 26 00 to 26 00 for option...</td>\n",
              "      <td>NI</td>\n",
              "      <td>2018-07-09</td>\n",
              "      <td>True</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.160000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>Mon Jul 09 00:17:21 +0000 2018</td>\n",
              "      <td>RT  TradeSatoshi   AMG AMGCHAIN  will be delis...</td>\n",
              "      <td>AMG</td>\n",
              "      <td>2018-07-09</td>\n",
              "      <td>True</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>Mon Jul 09 00:17:35 +0000 2018</td>\n",
              "      <td>Payments halted under Obamacare program    UNH...</td>\n",
              "      <td>HCA</td>\n",
              "      <td>2018-07-09</td>\n",
              "      <td>True</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>Mon Jul 09 00:21:28 +0000 2018</td>\n",
              "      <td>RT  forever t 2000  Theres goes  SODE breaking...</td>\n",
              "      <td>JEC</td>\n",
              "      <td>2018-07-09</td>\n",
              "      <td>True</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.300000</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>Mon Jul 09 00:31:10 +0000 2018</td>\n",
              "      <td>JEC Max Pain 65 00  Maturity 07 20 2018   max...</td>\n",
              "      <td>JEC</td>\n",
              "      <td>2018-07-09</td>\n",
              "      <td>True</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28259</th>\n",
              "      <td>Wed Jul 18 23:46:13 +0000 2018</td>\n",
              "      <td>FB   29234a9c 7f08 4d5a 985f cb1a5554ecf9</td>\n",
              "      <td>FB</td>\n",
              "      <td>2018-07-18</td>\n",
              "      <td>True</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28261</th>\n",
              "      <td>Wed Jul 18 23:46:19 +0000 2018</td>\n",
              "      <td>RT  invest in hd   Nuff said    TEL  telcoin  ...</td>\n",
              "      <td>BTC</td>\n",
              "      <td>2018-07-18</td>\n",
              "      <td>True</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28260</th>\n",
              "      <td>Wed Jul 18 23:46:19 +0000 2018</td>\n",
              "      <td>BTC</td>\n",
              "      <td>BTC</td>\n",
              "      <td>2018-07-18</td>\n",
              "      <td>True</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28262</th>\n",
              "      <td>Wed Jul 18 23:46:20 +0000 2018</td>\n",
              "      <td>BTC</td>\n",
              "      <td>BTC</td>\n",
              "      <td>2018-07-18</td>\n",
              "      <td>True</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28263</th>\n",
              "      <td>Wed Jul 18 23:46:27 +0000 2018</td>\n",
              "      <td>Stellar  XLM price   0 297852 Binance registra...</td>\n",
              "      <td>AMP</td>\n",
              "      <td>2018-07-18</td>\n",
              "      <td>True</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.059524</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>22487 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                            timestamp  ... Emotion\n",
              "47     Mon Jul 09 00:15:27 +0000 2018  ...       1\n",
              "48     Mon Jul 09 00:17:21 +0000 2018  ...       0\n",
              "49     Mon Jul 09 00:17:35 +0000 2018  ...       0\n",
              "50     Mon Jul 09 00:21:28 +0000 2018  ...       2\n",
              "51     Mon Jul 09 00:31:10 +0000 2018  ...       0\n",
              "...                               ...  ...     ...\n",
              "28259  Wed Jul 18 23:46:13 +0000 2018  ...       0\n",
              "28261  Wed Jul 18 23:46:19 +0000 2018  ...       0\n",
              "28260  Wed Jul 18 23:46:19 +0000 2018  ...       0\n",
              "28262  Wed Jul 18 23:46:20 +0000 2018  ...       0\n",
              "28263  Wed Jul 18 23:46:27 +0000 2018  ...       1\n",
              "\n",
              "[22487 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzMRm45fao_V"
      },
      "source": [
        "# Percentage of each Emotions overall symbols\n",
        "\n",
        "df_neutral   = df_cleaned['text'][df_cleaned['Emotion'] == '0']\n",
        "df_positive  = df_cleaned['text'][df_cleaned['Emotion'] == '1']\n",
        "df_negative  = df_cleaned['text'][df_cleaned['Emotion'] == '2']"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1khHii0a42j",
        "outputId": "c192d6dd-8723-4e2d-bbd6-dfb8eee2fd3e"
      },
      "source": [
        "print(f'Percentage Positive: {len(df_positive)/len(df_cleaned)}')\n",
        "print(f'Percentage Negetive: {len(df_negative)/len(df_cleaned)}')\n",
        "print(f'Percentage Neutral: {len(df_neutral)/len(df_cleaned)}')"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percentage Positive: 0.2766932005158536\n",
            "Percentage Negetive: 0.12304887268199403\n",
            "Percentage Neutral: 0.6002579268021524\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ni8yyKnKbwOJ"
      },
      "source": [
        "# NLP Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jW5a7xmCbzi5"
      },
      "source": [
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from nltk import ngrams\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import svm\n",
        "from sklearn import tree\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(df_cleaned['text'], df_cleaned['Emotion'], test_size=0.2, random_state=50)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yp83__U6b82x"
      },
      "source": [
        "# Ngram Models\n",
        "def NgramModels(Model , txt, n):\n",
        "    vect      = CountVectorizer(max_features=1000 , ngram_range=(n,n))\n",
        "    train_vect= vect.fit_transform(x_train)\n",
        "    test_vect = vect.transform(x_test)\n",
        "    \n",
        "    model     = Model\n",
        "    t0        = time.time()\n",
        "    model.fit(train_vect, y_train)\n",
        "    t1        = time.time()\n",
        "    predicted = model.predict(test_vect)\n",
        "    t2        = time.time()\n",
        "    time_train= t1-t0\n",
        "    time_pred = t2-t1\n",
        "    \n",
        "    accuracy  = model.score(train_vect, y_train)\n",
        "    predicted = model.predict(test_vect)\n",
        "    \n",
        "    report = classification_report(y_test, predicted, output_dict=True)\n",
        "    print(\"Models with \" , n , \"-grams :\\n\")\n",
        "    print('********************** \\n')\n",
        "    print(txt)\n",
        "    print(\"Training time: %fs; Prediction time: %fs \\n\" % (time_train, time_pred))\n",
        "    print('Accuracy score train set :', accuracy)\n",
        "    print('Accuracy score test set  :', accuracy_score(y_test, predicted),'\\n')\n",
        "    print('Positive: ', report['1'])\n",
        "    print('Neutral : ', report['0'])\n",
        "    print('Negative: ', report['2'])\n",
        "    print('\\n --------------------------------------------------------------------------------------------------- \\n')"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBIsWpxOclwz"
      },
      "source": [
        "def KNN_Ngram(n):\n",
        "    vect      = CountVectorizer(max_features=1000 , ngram_range=(n,n))\n",
        "    train_vect= vect.fit_transform(x_train)\n",
        "    test_vect = vect.transform(x_test)\n",
        "    \n",
        "    for k in [1,3,5,7,10]:\n",
        "        model = KNeighborsClassifier(n_neighbors=k,algorithm='brute')\n",
        "        t0        = time.time()\n",
        "        model.fit(train_vect, y_train)\n",
        "        t1        = time.time()\n",
        "        predicted = model.predict(test_vect)\n",
        "        t2        = time.time()\n",
        "        time_train= t1-t0\n",
        "        time_pred = t2-t1\n",
        "\n",
        "        accuracy  = model.score(train_vect, y_train)\n",
        "        predicted = model.predict(test_vect)\n",
        "\n",
        "        report = classification_report(y_test, predicted, output_dict=True)\n",
        "\n",
        "        print(\"Models with \" , n , \"-grams :\\n\")\n",
        "        print('********************** \\n')\n",
        "        print(\"Classification Report for k = {} is:\\n\".format(k))\n",
        "        print(\"Training time: %fs ; Prediction time: %fs \\n\" % (time_train, time_pred))\n",
        "        print('Accuracy score train set :', accuracy)\n",
        "        print('Accuracy score test set  :', accuracy_score(y_test, predicted),'\\n')\n",
        "        print('Positive: ', report['1'])\n",
        "        print('Neutral : ', report['0'])\n",
        "        print('Negative: ', report['2'])\n",
        "        print('\\n -------------------------------------------------------------------------------------- \\n')"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJcS2cEXdHFX"
      },
      "source": [
        "def TFIDFModels(Model,txt):\n",
        "    vect      = TfidfVectorizer(min_df = 5, max_df =0.8, sublinear_tf = True, use_idf = True)\n",
        "    train_vect= vect.fit_transform(x_train)\n",
        "    test_vect = vect.transform(x_test)\n",
        "    \n",
        "    model     = Model\n",
        "    t0        = time.time()\n",
        "    model.fit(train_vect, y_train)\n",
        "    t1        = time.time()\n",
        "    predicted = model.predict(test_vect)\n",
        "    t2        = time.time()\n",
        "    time_train= t1-t0\n",
        "    time_pred = t2-t1\n",
        "    \n",
        "    accuracy  = model.score(train_vect, y_train)\n",
        "    predicted = model.predict(test_vect)\n",
        "    \n",
        "    report = classification_report(y_test, predicted, output_dict=True)\n",
        "    \n",
        "    print(txt)\n",
        "    print(\"Training time: %fs; Prediction time: %fs \\n\" % (time_train, time_pred))\n",
        "    print('Accuracy score train set :', accuracy)\n",
        "    print('Accuracy score test set  :', accuracy_score(y_test, predicted),'\\n')\n",
        "    print('Positive: ', report['1'])\n",
        "    print('Neutral : ', report['0'])\n",
        "    print('Negative: ', report['2'])\n",
        "    print('\\n -------------------------------------------------------------------------------------- \\n')"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKLeIM__dMv7"
      },
      "source": [
        "def KNN_TFIDF():\n",
        "    vect      = TfidfVectorizer(min_df = 5, max_df =0.8, sublinear_tf = True, use_idf = True)\n",
        "    train_vect= vect.fit_transform(x_train)\n",
        "    test_vect = vect.transform(x_test)\n",
        "    \n",
        "    for k in [1,3,5,7,10]:\n",
        "\n",
        "        model = KNeighborsClassifier(n_neighbors=k,algorithm='brute')\n",
        "        t0        = time.time()\n",
        "        model.fit(train_vect, y_train)\n",
        "        t1        = time.time()\n",
        "        predicted = model.predict(test_vect)\n",
        "        t2        = time.time()\n",
        "        time_train= t1-t0\n",
        "        time_pred = t2-t1\n",
        "\n",
        "        accuracy  = model.score(train_vect, y_train)\n",
        "        predicted = model.predict(test_vect)\n",
        "\n",
        "        report = classification_report(y_test, predicted, output_dict=True)\n",
        "\n",
        "        print(\"Classification Report for k = {} is:\\n\".format(k))\n",
        "        print(\"Training time: %fs ; Prediction time: %fs \\n\" % (time_train, time_pred))\n",
        "        print('Accuracy score train set :', accuracy)\n",
        "        print('Accuracy score test set  :', accuracy_score(y_test, predicted),'\\n')\n",
        "        print('Positive: ', report['1'])\n",
        "        print('Neutral : ', report['0'])\n",
        "        print('Negative: ', report['2'])\n",
        "        print('\\n -------------------------------------------------------------------------------------- \\n')\n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyxLfIgleAks"
      },
      "source": [
        "# Train Models and Make Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cusGiMrd_8O",
        "outputId": "79ccc30e-2242-4e00-cbf2-8c01b646b882"
      },
      "source": [
        "SupportVectorClassifier=svm.SVC(kernel='linear')\n",
        "\n",
        "LogReg2 = NgramModels(Model = LogisticRegression(),txt = '\\nLogistic Regression Model : \\n ', n=2)\n",
        "LogReg3 = NgramModels(Model = LogisticRegression(),txt = 'Logistic Regression Model : \\n ', n=3)\n",
        "\n",
        "svm2 = NgramModels(Model = SupportVectorClassifier ,txt = 'Support Vectoer Classifier Model : \\n ', n=2)\n",
        "svm3 = NgramModels(Model = SupportVectorClassifier ,txt = 'Support Vectoer Classifier Model : \\n ', n=3)\n",
        "\n",
        "DecTree2 = NgramModels(Model = tree.DecisionTreeClassifier(),txt = 'Decision Tree Classifier Model : \\n ', n=2)\n",
        "DecTree3 = NgramModels(Model = tree.DecisionTreeClassifier(),txt = 'Decision Tree Classifier Model : \\n ', n=3)\n",
        "\n",
        "KNN2=KNN_Ngram(2)\n",
        "KNN3=KNN_Ngram(3)\n",
        "\n",
        "print('Models with Tfidf Feature extraction Techniques : \\n')\n",
        "print('************************************************ \\n')\n",
        "\n",
        "LogReg = TFIDFModels(Model = LogisticRegression(),txt = 'Logistic Regression Model : \\n ')\n",
        "svm = TFIDFModels(Model = SupportVectorClassifier,txt = 'Support Vector Classifier Model : \\n ')\n",
        "DecTree = TFIDFModels(Model = tree.DecisionTreeClassifier(),txt = 'Decision Tree Classifier Model : \\n ')\n",
        "knn_tfidf = KNN_TFIDF()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models with  2 -grams :\n",
            "\n",
            "********************** \n",
            "\n",
            "\n",
            "Logistic Regression Model : \n",
            " \n",
            "Training time: 0.887120s; Prediction time: 0.000559s \n",
            "\n",
            "Accuracy score train set : 0.7908721996775807\n",
            "Accuracy score test set  : 0.7839039573143619 \n",
            "\n",
            "Positive:  {'precision': 0.8270348837209303, 'recall': 0.47774979009235935, 'f1-score': 0.6056412985630655, 'support': 1191}\n",
            "Neutral :  {'precision': 0.7602996254681648, 'recall': 0.9627873039036848, 'f1-score': 0.8496458467482292, 'support': 2741}\n",
            "Negative:  {'precision': 0.9380530973451328, 'recall': 0.5618374558303887, 'f1-score': 0.7027624309392264, 'support': 566}\n",
            "\n",
            " --------------------------------------------------------------------------------------------------- \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models with  3 -grams :\n",
            "\n",
            "********************** \n",
            "\n",
            "Logistic Regression Model : \n",
            " \n",
            "Training time: 0.764985s; Prediction time: 0.000448s \n",
            "\n",
            "Accuracy score train set : 0.7327811440324643\n",
            "Accuracy score test set  : 0.7354379724321921 \n",
            "\n",
            "Positive:  {'precision': 0.9388297872340425, 'recall': 0.2963895885810244, 'f1-score': 0.45054243777919595, 'support': 1191}\n",
            "Neutral :  {'precision': 0.7015503875968992, 'recall': 0.9905144107989785, 'f1-score': 0.8213583421570111, 'support': 2741}\n",
            "Negative:  {'precision': 0.9523809523809523, 'recall': 0.42402826855123676, 'f1-score': 0.5867970660146699, 'support': 566}\n",
            "\n",
            " --------------------------------------------------------------------------------------------------- \n",
            "\n",
            "Models with  2 -grams :\n",
            "\n",
            "********************** \n",
            "\n",
            "Support Vectoer Classifier Model : \n",
            " \n",
            "Training time: 8.981488s; Prediction time: 0.734433s \n",
            "\n",
            "Accuracy score train set : 0.7904830729890489\n",
            "Accuracy score test set  : 0.785460204535349 \n",
            "\n",
            "Positive:  {'precision': 0.8413173652694611, 'recall': 0.471872376154492, 'f1-score': 0.6046261430876816, 'support': 1191}\n",
            "Neutral :  {'precision': 0.7603211009174312, 'recall': 0.9675300985041956, 'f1-score': 0.8515010435061808, 'support': 2741}\n",
            "Negative:  {'precision': 0.9327485380116959, 'recall': 0.5636042402826855, 'f1-score': 0.7026431718061675, 'support': 566}\n",
            "\n",
            " --------------------------------------------------------------------------------------------------- \n",
            "\n",
            "Models with  3 -grams :\n",
            "\n",
            "********************** \n",
            "\n",
            "Support Vectoer Classifier Model : \n",
            " \n",
            "Training time: 3.333933s; Prediction time: 0.399693s \n",
            "\n",
            "Accuracy score train set : 0.7317249430207349\n",
            "Accuracy score test set  : 0.7358826144953312 \n",
            "\n",
            "Positive:  {'precision': 0.9366754617414248, 'recall': 0.2980688497061293, 'f1-score': 0.45222929936305734, 'support': 1191}\n",
            "Neutral :  {'precision': 0.7017272492910543, 'recall': 0.9930682232761766, 'f1-score': 0.8223564954682779, 'support': 2741}\n",
            "Negative:  {'precision': 0.9708333333333333, 'recall': 0.411660777385159, 'f1-score': 0.5781637717121588, 'support': 566}\n",
            "\n",
            " --------------------------------------------------------------------------------------------------- \n",
            "\n",
            "Models with  2 -grams :\n",
            "\n",
            "********************** \n",
            "\n",
            "Decision Tree Classifier Model : \n",
            " \n",
            "Training time: 0.224096s; Prediction time: 0.003798s \n",
            "\n",
            "Accuracy score train set : 0.8259491911723831\n",
            "Accuracy score test set  : 0.7954646509559804 \n",
            "\n",
            "Positive:  {'precision': 0.8040712468193384, 'recall': 0.5306465155331654, 'f1-score': 0.6393525543753162, 'support': 1191}\n",
            "Neutral :  {'precision': 0.7792712066905615, 'recall': 0.9518423932871215, 'f1-score': 0.8569551650517326, 'support': 2741}\n",
            "Negative:  {'precision': 0.9258241758241759, 'recall': 0.5954063604240283, 'f1-score': 0.7247311827956989, 'support': 566}\n",
            "\n",
            " --------------------------------------------------------------------------------------------------- \n",
            "\n",
            "Models with  3 -grams :\n",
            "\n",
            "********************** \n",
            "\n",
            "Decision Tree Classifier Model : \n",
            " \n",
            "Training time: 0.109860s; Prediction time: 0.003202s \n",
            "\n",
            "Accuracy score train set : 0.7365612318639169\n",
            "Accuracy score test set  : 0.7354379724321921 \n",
            "\n",
            "Positive:  {'precision': 0.9149484536082474, 'recall': 0.2980688497061293, 'f1-score': 0.4496516782773907, 'support': 1191}\n",
            "Neutral :  {'precision': 0.703425012973534, 'recall': 0.9890550893834367, 'f1-score': 0.8221379833206975, 'support': 2741}\n",
            "Negative:  {'precision': 0.9453125, 'recall': 0.4275618374558304, 'f1-score': 0.5888077858880779, 'support': 566}\n",
            "\n",
            " --------------------------------------------------------------------------------------------------- \n",
            "\n",
            "Models with  2 -grams :\n",
            "\n",
            "********************** \n",
            "\n",
            "Classification Report for k = 1 is:\n",
            "\n",
            "Training time: 0.014738s ; Prediction time: 1.208513s \n",
            "\n",
            "Accuracy score train set : 0.69197843126355\n",
            "Accuracy score test set  : 0.6520675855935971 \n",
            "\n",
            "Positive:  {'precision': 0.443004023245418, 'recall': 0.8320738874895046, 'f1-score': 0.5781796966161027, 'support': 1191}\n",
            "Neutral :  {'precision': 0.8730583824317086, 'recall': 0.5946734768332725, 'f1-score': 0.7074652777777779, 'support': 2741}\n",
            "Negative:  {'precision': 0.7918781725888325, 'recall': 0.5512367491166078, 'f1-score': 0.65, 'support': 566}\n",
            "\n",
            " -------------------------------------------------------------------------------------- \n",
            "\n",
            "Models with  2 -grams :\n",
            "\n",
            "********************** \n",
            "\n",
            "Classification Report for k = 3 is:\n",
            "\n",
            "Training time: 0.023996s ; Prediction time: 1.122999s \n",
            "\n",
            "Accuracy score train set : 0.6813052420923897\n",
            "Accuracy score test set  : 0.6594041796353936 \n",
            "\n",
            "Positive:  {'precision': 0.44829199812821713, 'recall': 0.8043660789252729, 'f1-score': 0.5757211538461539, 'support': 1191}\n",
            "Neutral :  {'precision': 0.8476426799007444, 'recall': 0.6231302444363371, 'f1-score': 0.71825063078217, 'support': 2741}\n",
            "Negative:  {'precision': 0.8670520231213873, 'recall': 0.5300353356890459, 'f1-score': 0.6578947368421052, 'support': 566}\n",
            "\n",
            " -------------------------------------------------------------------------------------- \n",
            "\n",
            "Models with  2 -grams :\n",
            "\n",
            "********************** \n",
            "\n",
            "Classification Report for k = 5 is:\n",
            "\n",
            "Training time: 0.020550s ; Prediction time: 1.532136s \n",
            "\n",
            "Accuracy score train set : 0.7875924175885263\n",
            "Accuracy score test set  : 0.7681191640729212 \n",
            "\n",
            "Positive:  {'precision': 0.7800546448087432, 'recall': 0.4794290512174643, 'f1-score': 0.593863754550182, 'support': 1191}\n",
            "Neutral :  {'precision': 0.7536189924724956, 'recall': 0.9496534111638089, 'f1-score': 0.8403551251008878, 'support': 2741}\n",
            "Negative:  {'precision': 0.9006410256410257, 'recall': 0.49646643109540634, 'f1-score': 0.6400911161731208, 'support': 566}\n",
            "\n",
            " -------------------------------------------------------------------------------------- \n",
            "\n",
            "Models with  2 -grams :\n",
            "\n",
            "********************** \n",
            "\n",
            "Classification Report for k = 7 is:\n",
            "\n",
            "Training time: 0.020988s ; Prediction time: 1.546908s \n",
            "\n",
            "Accuracy score train set : 0.7818666963144144\n",
            "Accuracy score test set  : 0.7672298799466429 \n",
            "\n",
            "Positive:  {'precision': 0.791023842917251, 'recall': 0.473551637279597, 'f1-score': 0.5924369747899159, 'support': 1191}\n",
            "Neutral :  {'precision': 0.7515831894070236, 'recall': 0.9525720539948924, 'f1-score': 0.8402252614641995, 'support': 2741}\n",
            "Negative:  {'precision': 0.887459807073955, 'recall': 0.4876325088339223, 'f1-score': 0.6294184720638542, 'support': 566}\n",
            "\n",
            " -------------------------------------------------------------------------------------- \n",
            "\n",
            "Models with  2 -grams :\n",
            "\n",
            "********************** \n",
            "\n",
            "Classification Report for k = 10 is:\n",
            "\n",
            "Training time: 0.027997s ; Prediction time: 1.468334s \n",
            "\n",
            "Accuracy score train set : 0.7766968703096336\n",
            "Accuracy score test set  : 0.7656736327256558 \n",
            "\n",
            "Positive:  {'precision': 0.8020833333333334, 'recall': 0.45256087321578503, 'f1-score': 0.5786366076221149, 'support': 1191}\n",
            "Neutral :  {'precision': 0.7465346534653465, 'recall': 0.9627873039036848, 'f1-score': 0.8409815168897387, 'support': 2741}\n",
            "Negative:  {'precision': 0.9140893470790378, 'recall': 0.46996466431095407, 'f1-score': 0.6207701283547258, 'support': 566}\n",
            "\n",
            " -------------------------------------------------------------------------------------- \n",
            "\n",
            "Models with  3 -grams :\n",
            "\n",
            "********************** \n",
            "\n",
            "Classification Report for k = 1 is:\n",
            "\n",
            "Training time: 0.013801s ; Prediction time: 0.867145s \n",
            "\n",
            "Accuracy score train set : 0.5586191561509811\n",
            "Accuracy score test set  : 0.5375722543352601 \n",
            "\n",
            "Positive:  {'precision': 0.3656473649967469, 'recall': 0.943744752308984, 'f1-score': 0.5270808909730363, 'support': 1191}\n",
            "Neutral :  {'precision': 0.9269366197183099, 'recall': 0.3841663626413718, 'f1-score': 0.5432035078669074, 'support': 2741}\n",
            "Negative:  {'precision': 0.8368055555555556, 'recall': 0.42579505300353354, 'f1-score': 0.5644028103044496, 'support': 566}\n",
            "\n",
            " -------------------------------------------------------------------------------------- \n",
            "\n",
            "Models with  3 -grams :\n",
            "\n",
            "********************** \n",
            "\n",
            "Classification Report for k = 3 is:\n",
            "\n",
            "Training time: 0.020273s ; Prediction time: 1.104571s \n",
            "\n",
            "Accuracy score train set : 0.5613430429707044\n",
            "Accuracy score test set  : 0.5429079590929302 \n",
            "\n",
            "Positive:  {'precision': 0.3660389824909151, 'recall': 0.9303106633081444, 'f1-score': 0.5253674727358938, 'support': 1191}\n",
            "Neutral :  {'precision': 0.90089358245329, 'recall': 0.4045968624589566, 'f1-score': 0.5584088620342397, 'support': 2741}\n",
            "Negative:  {'precision': 0.9375, 'recall': 0.39752650176678445, 'f1-score': 0.5583126550868487, 'support': 566}\n",
            "\n",
            " -------------------------------------------------------------------------------------- \n",
            "\n",
            "Models with  3 -grams :\n",
            "\n",
            "********************** \n",
            "\n",
            "Classification Report for k = 5 is:\n",
            "\n",
            "Training time: 0.019588s ; Prediction time: 1.327281s \n",
            "\n",
            "Accuracy score train set : 0.7288898771471455\n",
            "Accuracy score test set  : 0.7281013783903957 \n",
            "\n",
            "Positive:  {'precision': 0.9086161879895561, 'recall': 0.29219143576826195, 'f1-score': 0.44218551461245237, 'support': 1191}\n",
            "Neutral :  {'precision': 0.6977823620422898, 'recall': 0.9872309376140095, 'f1-score': 0.8176461701163317, 'support': 2741}\n",
            "Negative:  {'precision': 0.9324894514767933, 'recall': 0.39045936395759717, 'f1-score': 0.5504358655043586, 'support': 566}\n",
            "\n",
            " -------------------------------------------------------------------------------------- \n",
            "\n",
            "Models with  3 -grams :\n",
            "\n",
            "********************** \n",
            "\n",
            "Classification Report for k = 7 is:\n",
            "\n",
            "Training time: 0.019651s ; Prediction time: 1.334035s \n",
            "\n",
            "Accuracy score train set : 0.7269998332314191\n",
            "Accuracy score test set  : 0.7278790573588262 \n",
            "\n",
            "Positive:  {'precision': 0.9204244031830239, 'recall': 0.29135180520570947, 'f1-score': 0.44260204081632654, 'support': 1191}\n",
            "Neutral :  {'precision': 0.6970164609053497, 'recall': 0.9886902590295512, 'f1-score': 0.8176195504600995, 'support': 2741}\n",
            "Negative:  {'precision': 0.9313304721030042, 'recall': 0.3833922261484099, 'f1-score': 0.5431789737171464, 'support': 566}\n",
            "\n",
            " -------------------------------------------------------------------------------------- \n",
            "\n",
            "Models with  3 -grams :\n",
            "\n",
            "********************** \n",
            "\n",
            "Classification Report for k = 10 is:\n",
            "\n",
            "Training time: 0.019230s ; Prediction time: 1.317122s \n",
            "\n",
            "Accuracy score train set : 0.7256656845850242\n",
            "Accuracy score test set  : 0.7272120942641174 \n",
            "\n",
            "Positive:  {'precision': 0.9095744680851063, 'recall': 0.2871536523929471, 'f1-score': 0.43650287172941926, 'support': 1191}\n",
            "Neutral :  {'precision': 0.6967907573812581, 'recall': 0.990149580445093, 'f1-score': 0.8179626280892104, 'support': 2741}\n",
            "Negative:  {'precision': 0.947136563876652, 'recall': 0.37985865724381623, 'f1-score': 0.5422446406052963, 'support': 566}\n",
            "\n",
            " -------------------------------------------------------------------------------------- \n",
            "\n",
            "Models with Tfidf Feature extraction Techniques : \n",
            "\n",
            "************************************************ \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Model : \n",
            " \n",
            "Training time: 2.148039s; Prediction time: 0.001040s \n",
            "\n",
            "Accuracy score train set : 0.9578075490577576\n",
            "Accuracy score test set  : 0.9339706536238328 \n",
            "\n",
            "Positive:  {'precision': 0.949685534591195, 'recall': 0.887489504617968, 'f1-score': 0.9175347222222222, 'support': 1191}\n",
            "Neutral :  {'precision': 0.9214358279715543, 'recall': 0.9927033929222912, 'f1-score': 0.9557428872497366, 'support': 2741}\n",
            "Negative:  {'precision': 0.9791666666666666, 'recall': 0.7473498233215548, 'f1-score': 0.8476953907815632, 'support': 566}\n",
            "\n",
            " -------------------------------------------------------------------------------------- \n",
            "\n",
            "Support Vector Classifier Model : \n",
            " \n",
            "Training time: 24.267497s; Prediction time: 3.619398s \n",
            "\n",
            "Accuracy score train set : 0.9783756740230141\n",
            "Accuracy score test set  : 0.9573143619386394 \n",
            "\n",
            "Positive:  {'precision': 0.9718061674008811, 'recall': 0.926112510495382, 'f1-score': 0.9484092863284609, 'support': 1191}\n",
            "Neutral :  {'precision': 0.9488695652173913, 'recall': 0.9952572053994893, 'f1-score': 0.9715099715099714, 'support': 2741}\n",
            "Negative:  {'precision': 0.9733606557377049, 'recall': 0.8392226148409894, 'f1-score': 0.9013282732447819, 'support': 566}\n",
            "\n",
            " -------------------------------------------------------------------------------------- \n",
            "\n",
            "Decision Tree Classifier Model : \n",
            " \n",
            "Training time: 3.949574s; Prediction time: 0.003888s \n",
            "\n",
            "Accuracy score train set : 0.9999444104730669\n",
            "Accuracy score test set  : 0.9466429524232992 \n",
            "\n",
            "Positive:  {'precision': 0.9348561759729273, 'recall': 0.927791771620487, 'f1-score': 0.9313105773282764, 'support': 1191}\n",
            "Neutral :  {'precision': 0.9638467100506146, 'recall': 0.9726377234585918, 'f1-score': 0.9682222625749047, 'support': 2741}\n",
            "Negative:  {'precision': 0.8854545454545455, 'recall': 0.8604240282685512, 'f1-score': 0.8727598566308243, 'support': 566}\n",
            "\n",
            " -------------------------------------------------------------------------------------- \n",
            "\n",
            "Classification Report for k = 1 is:\n",
            "\n",
            "Training time: 0.014141s ; Prediction time: 1.284858s \n",
            "\n",
            "Accuracy score train set : 0.9999444104730669\n",
            "Accuracy score test set  : 0.8268119164072921 \n",
            "\n",
            "Positive:  {'precision': 0.9417360285374554, 'recall': 0.6649874055415617, 'f1-score': 0.7795275590551181, 'support': 1191}\n",
            "Neutral :  {'precision': 0.8140560983296565, 'recall': 0.9423568040861, 'f1-score': 0.8735204599256003, 'support': 2741}\n",
            "Negative:  {'precision': 0.7107438016528925, 'recall': 0.607773851590106, 'f1-score': 0.6552380952380952, 'support': 566}\n",
            "\n",
            " -------------------------------------------------------------------------------------- \n",
            "\n",
            "Classification Report for k = 3 is:\n",
            "\n",
            "Training time: 0.021028s ; Prediction time: 1.512460s \n",
            "\n",
            "Accuracy score train set : 0.8602479292901217\n",
            "Accuracy score test set  : 0.7810137839039573 \n",
            "\n",
            "Positive:  {'precision': 0.9493293591654247, 'recall': 0.5348446683459278, 'f1-score': 0.6842105263157894, 'support': 1191}\n",
            "Neutral :  {'precision': 0.7564362163725774, 'recall': 0.9540313754104341, 'f1-score': 0.8438205872862213, 'support': 2741}\n",
            "Negative:  {'precision': 0.7054054054054054, 'recall': 0.46113074204946997, 'f1-score': 0.5576923076923076, 'support': 566}\n",
            "\n",
            " -------------------------------------------------------------------------------------- \n",
            "\n",
            "Classification Report for k = 5 is:\n",
            "\n",
            "Training time: 0.022029s ; Prediction time: 1.862504s \n",
            "\n",
            "Accuracy score train set : 0.8035466118183334\n",
            "Accuracy score test set  : 0.7581147176522899 \n",
            "\n",
            "Positive:  {'precision': 0.9448275862068966, 'recall': 0.46011754827875734, 'f1-score': 0.6188594014680971, 'support': 1191}\n",
            "Neutral :  {'precision': 0.7278187278187278, 'recall': 0.9726377234585918, 'f1-score': 0.8326046221111806, 'support': 2741}\n",
            "Negative:  {'precision': 0.7686274509803922, 'recall': 0.3462897526501767, 'f1-score': 0.4774665042630938, 'support': 566}\n",
            "\n",
            " -------------------------------------------------------------------------------------- \n",
            "\n",
            "Classification Report for k = 7 is:\n",
            "\n",
            "Training time: 0.019353s ; Prediction time: 1.868849s \n",
            "\n",
            "Accuracy score train set : 0.7691366946467285\n",
            "Accuracy score test set  : 0.740329035126723 \n",
            "\n",
            "Positive:  {'precision': 0.9591836734693877, 'recall': 0.39462636439966414, 'f1-score': 0.5591909577632361, 'support': 1191}\n",
            "Neutral :  {'precision': 0.7109933774834437, 'recall': 0.9792046698285297, 'f1-score': 0.8238182934315531, 'support': 2741}\n",
            "Negative:  {'precision': 0.7553648068669528, 'recall': 0.31095406360424027, 'f1-score': 0.4405506883604505, 'support': 566}\n",
            "\n",
            " -------------------------------------------------------------------------------------- \n",
            "\n",
            "Classification Report for k = 10 is:\n",
            "\n",
            "Training time: 0.019441s ; Prediction time: 1.897039s \n",
            "\n",
            "Accuracy score train set : 0.7284451609316804\n",
            "Accuracy score test set  : 0.7174299688750556 \n",
            "\n",
            "Positive:  {'precision': 0.9670050761421319, 'recall': 0.3198992443324937, 'f1-score': 0.48075709779179804, 'support': 1191}\n",
            "Neutral :  {'precision': 0.6873737373737374, 'recall': 0.9930682232761766, 'f1-score': 0.81241605730488, 'support': 2741}\n",
            "Negative:  {'precision': 0.8611111111111112, 'recall': 0.21908127208480566, 'f1-score': 0.34929577464788736, 'support': 566}\n",
            "\n",
            " -------------------------------------------------------------------------------------- \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJnb-EZof_lG"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8P0ZgLzTf78x"
      },
      "source": [
        "idx = pd.MultiIndex.from_product([['2-grams', '3-grams', 'TFIDF'],['Accuracy Training %','Accuracy Testing %']],names=['FeatureExtraction', 'Metric'])\n",
        "col = ['LogisticRegression', 'SupportVectorClassifier', 'DecisionTree', 'KNeighborsClassifier']\n",
        "\n",
        "Result = pd.DataFrame('*', idx, col)\n",
        "\n",
        "Result.LogisticRegression=['79.17','76.92','73.60','71.99','95.73','93.35']\n",
        "Result.SupportVectorClassifier=['78.91','76.94','73.65','72.15','97.83','95.99']\n",
        "Result.DecisionTree=['82.66','77.32','74.12','72.15','1.0','95.96']\n",
        "Result.KNeighborsClassifier=['80.77','74.24','73.08','70.69','1.0','82.46']"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "id": "flJ_miQKkgqa",
        "outputId": "d1899003-1a53-448f-a5f3-5b11dfd2f1ba"
      },
      "source": [
        "Result"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>LogisticRegression</th>\n",
              "      <th>SupportVectorClassifier</th>\n",
              "      <th>DecisionTree</th>\n",
              "      <th>KNeighborsClassifier</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>FeatureExtraction</th>\n",
              "      <th>Metric</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">2-grams</th>\n",
              "      <th>Accuracy Training %</th>\n",
              "      <td>79.17</td>\n",
              "      <td>78.91</td>\n",
              "      <td>82.66</td>\n",
              "      <td>80.77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Accuracy Testing %</th>\n",
              "      <td>76.92</td>\n",
              "      <td>76.94</td>\n",
              "      <td>77.32</td>\n",
              "      <td>74.24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">3-grams</th>\n",
              "      <th>Accuracy Training %</th>\n",
              "      <td>73.60</td>\n",
              "      <td>73.65</td>\n",
              "      <td>74.12</td>\n",
              "      <td>73.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Accuracy Testing %</th>\n",
              "      <td>71.99</td>\n",
              "      <td>72.15</td>\n",
              "      <td>72.15</td>\n",
              "      <td>70.69</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">TFIDF</th>\n",
              "      <th>Accuracy Training %</th>\n",
              "      <td>95.73</td>\n",
              "      <td>97.83</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Accuracy Testing %</th>\n",
              "      <td>93.35</td>\n",
              "      <td>95.99</td>\n",
              "      <td>95.96</td>\n",
              "      <td>82.46</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      LogisticRegression  ... KNeighborsClassifier\n",
              "FeatureExtraction Metric                                  ...                     \n",
              "2-grams           Accuracy Training %              79.17  ...                80.77\n",
              "                  Accuracy Testing %               76.92  ...                74.24\n",
              "3-grams           Accuracy Training %              73.60  ...                73.08\n",
              "                  Accuracy Testing %               71.99  ...                70.69\n",
              "TFIDF             Accuracy Training %              95.73  ...                  1.0\n",
              "                  Accuracy Testing %               93.35  ...                82.46\n",
              "\n",
              "[6 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NNyZNJj7P0H"
      },
      "source": [
        "# Stock Price Predictions using sentiment analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "KKCFKxywkip_",
        "outputId": "eb8fc7b9-fc94-41d3-9726-aaf9b6813d9c"
      },
      "source": [
        "# columns to be used for prediction\n",
        "df_stock_val = combined_data[['Date','Price','Emotion','Negative','Neutral','Positive']].copy()\n",
        "df_stock_val"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Price</th>\n",
              "      <th>Emotion</th>\n",
              "      <th>Negative</th>\n",
              "      <th>Neutral</th>\n",
              "      <th>Positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2018-07-16</td>\n",
              "      <td>61</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2018-07-17</td>\n",
              "      <td>61</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2018-07-18</td>\n",
              "      <td>61</td>\n",
              "      <td>0.9657</td>\n",
              "      <td>0.033</td>\n",
              "      <td>0.887</td>\n",
              "      <td>0.9657</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2018-07-11</td>\n",
              "      <td>61</td>\n",
              "      <td>0.296</td>\n",
              "      <td>0</td>\n",
              "      <td>0.879</td>\n",
              "      <td>0.296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2018-07-12</td>\n",
              "      <td>61</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2303</th>\n",
              "      <td>2018-07-15</td>\n",
              "      <td>84</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2304</th>\n",
              "      <td>2018-07-16</td>\n",
              "      <td>82</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2305</th>\n",
              "      <td>2018-07-17</td>\n",
              "      <td>84</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2306</th>\n",
              "      <td>2018-07-18</td>\n",
              "      <td>84</td>\n",
              "      <td>-0.0258</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.943</td>\n",
              "      <td>-0.0258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2307</th>\n",
              "      <td>2018-07-12</td>\n",
              "      <td>84</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2308 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            Date  Price Emotion Negative Neutral Positive\n",
              "0     2018-07-16     61       0        0       1        0\n",
              "1     2018-07-17     61       0        0       1        0\n",
              "2     2018-07-18     61  0.9657    0.033   0.887   0.9657\n",
              "3     2018-07-11     61   0.296        0   0.879    0.296\n",
              "4     2018-07-12     61       0        0       1        0\n",
              "...          ...    ...     ...      ...     ...      ...\n",
              "2303  2018-07-15     84       0        0       1        0\n",
              "2304  2018-07-16     82       0        0       1        0\n",
              "2305  2018-07-17     84       0        0       1        0\n",
              "2306  2018-07-18     84 -0.0258    0.025   0.943  -0.0258\n",
              "2307  2018-07-12     84       0        0       1        0\n",
              "\n",
              "[2308 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JS_XIeOM75kr"
      },
      "source": [
        "# Split dataset for training and testing\n",
        "x_train, x_test, y_train, y_test = train_test_split(df_stock_val['Price'], df_stock_val['Emotion'], test_size=0.2, random_state=50)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WID-VFOp88E1"
      },
      "source": [
        "ls_sentiments_score = []\n",
        "for date, row in x_train.iteritems():\n",
        "    sentiment_score = np.asarray([combined_data.loc[date, 'Emotion']])\n",
        "    ls_sentiments_score.append(sentiment_score)\n",
        "numpy_dataframe_train = np.asarray(ls_sentiments_score)\n"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-G12ngCP-7qy"
      },
      "source": [
        "ls_sentiments_score = []\n",
        "for date, row in x_test.iteritems():\n",
        "    sentiment_score = np.asarray([combined_data.loc[date, 'Emotion']])\n",
        "    ls_sentiments_score.append(sentiment_score)\n",
        "numpy_dataframe_test = np.asarray(ls_sentiments_score)\n"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bskJ9MR2_MuA"
      },
      "source": [
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "gxFrkgyW_NDH",
        "outputId": "2070b17d-a762-49bd-8406-2a8d32e645a3"
      },
      "source": [
        "# from treeinterpreter import treeinterpreter as ti\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "\n",
        "rf = RandomForestRegressor()\n",
        "rf.fit(numpy_dataframe_train, y_train)\n",
        "prediction=rf.predict(numpy_dataframe_test)\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "#idx = pd.date_range(test_data_start, test_data_end)\n",
        "predictions_df = pd.DataFrame(data=prediction[0:], columns=['Price'])\n",
        "predictions_df['Price'] = predictions_df['Price'].apply(np.int64)\n",
        "predictions_df['Price'] = predictions_df['Price'] + 4500\n",
        "predictions_df['actual_value'] = y_test\n",
        "predictions_df.columns = ['predicted_price', 'actual_price']\n",
        "predictions_df.plot()\n",
        "predictions_df['predicted_price'] = predictions_df['predicted_price'].apply(np.int64)\n",
        "y_test = y_test.apply(np.int64)\n",
        "#print(accuracy_score(test['adj_close_price'],predictions_df['predicted_price']))\n",
        "print(rf.score(numpy_dataframe_train, y_train))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9999984586900995\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJ7B0WHD_xhf"
      },
      "source": [
        ""
      ],
      "execution_count": 57,
      "outputs": []
    }
  ]
}